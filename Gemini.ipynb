{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVaesAllh7KpnJoaTgqVE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InduwaraGayashan001/Generative-AI/blob/main/Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "2XyDa1rnxRI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhWBzdtlxKK-"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv langchain_experimental sentence-transformers langchain_chroma langchainhub unstructured"
      ],
      "metadata": {
        "id": "cXJ7PQGtxhJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "P4exdylrzjro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"Attention.pdf\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "N5M1znd4x8Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rbUjY7BuzXh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0X7_WLWzX_Y",
        "outputId": "271ca42e-a146-4232-9539-78b6447cb4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "dgRwxyg0zoeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(data)\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KddQF4HJzd5Y",
        "outputId": "da4bfeff-b485-42e6-9533-3e4a98ce3fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "mFLtDtWL0u0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "  model = \"models/embedding-001\"\n",
        ")\n",
        "\n",
        "vector = embeddings.embed_query(\"Hello World\")\n",
        "len(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX4X77s-z3pS",
        "outputId": "9a0f3e7a-dd77-401e-f41e-ddbcc012f97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Base"
      ],
      "metadata": {
        "id": "zr7QFYXc2_xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "pW7ADOkX3DWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type = \"similarity\",\n",
        "    search_kwargs = {\"k\": 2}\n",
        ")\n",
        "\n",
        "retrieved_docs = retriever.invoke(\"What is Attention Layer?\")\n",
        "len(retrieved_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orW0E-ND3dCW",
        "outputId": "86396bfc-f1c5-4769-d246-8d5c4b5bc749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5x4UoYl34yO",
        "outputId": "88353b61-b9f8-41ff-a500-cdec6131b2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
            "predictions for position ican depend only on the known outputs at positions less than i.\n",
            "3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrate with Gemini"
      ],
      "metadata": {
        "id": "IKKDDrvA4E0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    temperature = 0.2,\n",
        "    max_tokens = 1000,\n",
        ")"
      ],
      "metadata": {
        "id": "kBlsZswe4JlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an asistant for question answering tasks\"\n",
        "    \"Use the following pieces of retrieved context to answer the question.\"\n",
        "    \"If you don't know the answer, just say that you don't know.\"\n",
        "    \"Use 3 sentences maximum and keep the answer concise\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt), (\"human\", \"{input}\")])\n",
        "\n"
      ],
      "metadata": {
        "id": "8ThFc2Lo4l9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm,prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "TBIALNof5rmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"What is self attention?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgaa7cW85-zj",
        "outputId": "757d7024-d8f2-40f1-f5bf-4113f59806ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-attention, sometimes called intra-attention, is an attention mechanism that relates different positions of a single sequence. It computes a representation of the sequence and has been used in tasks like reading comprehension, abstractive summarization, and textual entailment. The Transformer is the first transduction model relying entirely on self-attention.\n"
          ]
        }
      ]
    }
  ]
}