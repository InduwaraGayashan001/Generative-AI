{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2KUETIuW08vqYr554qNZj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InduwaraGayashan001/Generative-AI/blob/main/Falcon_Multi-Doc_Retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "h8W0L7n_ZOyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-j287YdZLGR",
        "outputId": "72fa881a-06d4-4de8-c251-7d24915cdfd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.6.15)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.6/304.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip -q install langchain tiktoken chromadb pypdf transformers\n",
        "!pip -q install accelerate bitsandbytes sentencepiece Xformers\n",
        "!pip -q install langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "VMCZNNFKhiFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, DirectoryLoader"
      ],
      "metadata": {
        "id": "P8UIadD4acPq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Shafi2016/Youtube/raw/main/stock_market_june_2023.zip -O stock_market_june_2023.zip\n",
        "!unzip stock_market_june_2023.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FCT7pMkbEsk",
        "outputId": "49b05803-7a74-49b7-d7d2-7b98798bf8d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-22 16:21:41--  https://github.com/Shafi2016/Youtube/raw/main/stock_market_june_2023.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Shafi2016/Youtube/main/stock_market_june_2023.zip [following]\n",
            "--2025-06-22 16:21:41--  https://raw.githubusercontent.com/Shafi2016/Youtube/main/stock_market_june_2023.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10638 (10K) [application/zip]\n",
            "Saving to: ‘stock_market_june_2023.zip’\n",
            "\n",
            "stock_market_june_2 100%[===================>]  10.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-22 16:21:42 (114 MB/s) - ‘stock_market_june_2023.zip’ saved [10638/10638]\n",
            "\n",
            "Archive:  stock_market_june_2023.zip\n",
            "   creating: stock_market_june_2023/\n",
            "  inflating: stock_market_june_2023/Arbor Metals Corp.txt  \n",
            "  inflating: stock_market_june_2023/Intuitive Surgical.txt  \n",
            "  inflating: stock_market_june_2023/Kiplin Metals Inc.txt  \n",
            "  inflating: stock_market_june_2023/Mercado Libre.txt  \n",
            "  inflating: stock_market_june_2023/Microsoft Corporation.txt  \n",
            "  inflating: stock_market_june_2023/NVIDIA Corporation.txt  \n",
            "  inflating: stock_market_june_2023/Shopify.txt  \n",
            "  inflating: stock_market_june_2023/stock_market.txt  \n",
            "  inflating: stock_market_june_2023/Tesla Inc.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader('./stock_market_june_2023', glob='**/*.txt', loader_cls=TextLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "Ljs_0tQycn9Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxtT8yamcwTl",
        "outputId": "8f58fb96-d475-46f0-9861-a565c74565bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'stock_market_june_2023/Shopify.txt'}, page_content='Shopify (NYSE: SHOP) has 55% increase in its shares so far in 2023. Despite ongoing price fluctuations, Shopify’s business and growth trajectory are moving upward.\\nIn the seven years since its initial public offering, Shopify has been profitable in five, a commendable track record in an industry known for widespread unprofitability. The company has weathered economic volatility, including the challenges posed by the pandemic, by streamlining its focus on delivering market-leading software and solutions to empower its merchant clients.\\n\\nTo adapt to the changing landscape, Shopify implemented significant changes, including workforce layoffs of approximately 20% and selling its logistics business to Flexport. Although these changes incur near-term expenses, such as restructuring and impairment charges, they will ultimately reduce overhead costs and make Shopify more asset-light in the long run.\\n\\nIn Q1, Shopify returned to profitability and achieved positive free cash flow. Management expects the company to remain cash-flow-positive for the full year, highlighting its financial resilience.\\n\\nDespite not being commonly associated with AI, Shopify has recently introduced several AI products. For example, its machine-learning platform, Merlin, supports various use cases, from fraud detection and revenue predictions to product categorization and buyer recommendations. Shopify also integrated an intelligent chatbot called Shop.ai into its mobile app, serving as a consumer shopping assistant.\\n\\nFurthermore, Shopify Audiences, a marketing software utilizing machine learning, allows creating and exporting customer lists to platforms like Pinterest, Facebook, Instagram, and Google for targeted ad campaigns. The adoption of Shopify Audiences by merchants upgrading to Shopify Plus presents a promising growth opportunity.\\n\\nShopify’s ongoing commitment to innovation and expansion into AI and upmarket segments positions it for continued success and growth.\\n'),\n",
              " Document(metadata={'source': 'stock_market_june_2023/stock_market.txt'}, page_content='The stock market had a strong month in May, especially for those who read our previous monthly stock recommendation. At The Markets Watch, our journalists and analysts have compiled a list of monthly stock opportunities for buy-and-hold investors and traders looking to buy great companies at an even better price.\\nLooking at some of our stock suggestions from the previous month, the evidence of our team’s expertise becomes compelling. In May:\\n\\nPalantir soared by over 74%\\nNvidia witnessed a growth of over 34%\\nCrowdstrike Holdings increased by over 30%\\nShopify saw a rise of over 19%\\nOur insights and articles about these stocks collectively yielded a remarkable 57% return last month for our readers who took action on our recommendations.\\n\\nInstitutional ETFs assembled by Goldman Sachs and Vanguard rely on growth stocks to drive their price in all market conditions, and your portfolio is no different. Most of our recommendations are already a staple in these ETFs, but for investors looking to cherry-pick the best stocks, we have assembled this list for you.\\n\\nHere are 8 growth stocks for you to buy in June 2023 set to leverage emerging industry trends, such as AI, the lithium “gold” rush, blockbuster EV sales, and ESG.\\n\\nArbor Metals Corp.\\nTesla Inc.\\nKiplin Metals Inc.\\nMicrosoft\\nNVIDIA\\nShopify\\nIntuitive Surgical\\nMercadoLibre\\nGrowth stocks like these are ideal for investors seeking short-term gains and long-term resilience. These companies are primed to withstand future market fluctuations through innovation, market opportunities, embedding in recession-resistant sectors, and committing to diversifying their portfolios.\\n\\nFor our top pick, our analysts have issued a rare ‘Buy Now‘ alert as Arbor Metals Corp. (TSX-V: ABR, FWB: 432, OTC: ABRMF) is forecasted to soar exponentially in 2023. The Canadian mineral exploration company is on the verge of making a significant lithium discovery in one of its properties, just as the lithium industry braces itself for a bullish surge.'),\n",
              " Document(metadata={'source': 'stock_market_june_2023/NVIDIA Corporation.txt'}, page_content='Speaking of AI, NVIDIA (NASDAQ: NVDA) is already leading the charge to capitalize on this new megatrend. \\nWith the unveiling of OpenAI’s ChatGPT in late 2022, generative AI has moved from a tech buzzword to a powerful megatrend with the potential to revolutionize various industries by enhancing human productivity and creativity.\\nMarket analysts anticipate the AI sector to grow twentyfold, reaching nearly $2 trillion by the close of 2030 as the technology pervades domains from supply chains to mobile applications. If you equate this opportunity to the oil boom, Nvidia isn’t drilling for oil, but providing the drills and equipment necessary—a more secure and lucrative approach.\\n\\nNvidia dominates with an impressive 80% market share in advanced graphics processing units (GPUs), integral to training AI platforms like ChatGPT, which utilizes 10,000 of its A100 chips. As generative AI platforms advance and update, they will likely demand a larger volume of comparable hardware.\\n\\nWith each of Nvidia’s A100s retailing for approximately $10,000, an increase in AI-related demand could significantly bolster the company’s revenue as it amalgamates AI into its traditional business. However, Alphabet claims that a supercomputer powered by the company’s latest fourth-generation TPU outpaced Nvidia’s A100 by 1.7 times in speed while consuming less power.\\n\\nThis a moot point, as Alphabet could be more of an opportunity than a threat for Nvidia. In May, the search giant announced its A3 supercomputers engineered to assist clients in training the most demanding generative AI models. The catch? A3 relies on Nvidia’s H100 chips, reinforcing confidence in the GPU maker’s technological leadership. \\n\\nNvidia’s shares have seen a massive surge since the beginning of 2023, indicating that the market recognizes the company’s substantial potential in the AI industry. While some may consider the stock to be pricy, Nvidia has a lot of things going for it to help justify the premium price tag. \\n'),\n",
              " Document(metadata={'source': 'stock_market_june_2023/Microsoft Corporation.txt'}, page_content='Future-proofing your portfolio with growth stocks isn’t just about singling out products and industries that will always be in demand. \\nIt’s also about identifying companies engineered from inception to adapt to industry shifts. Microsoft (NASDAQ: MSFT) fits these criteria like a glove.\\nThe internet was still budding three decades ago, while cloud computing wasn’t even a concept. As technology evolved, so did Microsoft. The company now excels in various fields, including cloud computing, online ad management, business collaboration, video gaming, and innovations and investments related to AI. While still vital, its Windows operating system segment has taken a backseat in light of the company’s recent successes.\\n\\nLet’s focus on Microsoft’s artificial intelligence (AI) prospects. As AI propels itself into the mainstream, it’s not surprising to see investors eager to fortify their portfolios with AI stocks. Microsoft’s recent $1 billion investment in OpenAI – the creator of ChatGPT and the AI image generator Dall-E- positions the stock as a prime candidate for investment.\\n\\nThis Microsoft-OpenAI partnership clearly conveys Microsoft’s intent and efforts to incorporate AI capabilities into its services. Starting with its cloud service, Azure – second only to Amazon Web Services (AWS) in global cloud market share and a key component of Microsoft’s business – Microsoft is showcasing its commitment to AI.\\n\\nMicrosoft’s strategic move to integrate OpenAI’s products within its platform positions the company as a preferred platform for organizations looking to develop AI models or incorporate them into their business operations. Microsoft’s focus on building AI tools as platforms is a savvy strategic decision that will likely broaden its already considerable enterprise customer base and service suite.\\n'),\n",
              " Document(metadata={'source': 'stock_market_june_2023/Tesla Inc.txt'}, page_content='In the context of an imminent lithium market boom, why not invest in the world’s leading EV producer instead of just its lithium suppliers? \\nDoubling down on investments in both lithium producers and consumers is a smart investment strategy, especially as lithium stocks are primed for \\nsignificant growth in the forthcoming years—a winning recipe to future-proof your portfolio.\\nIn March, Elon Musk unveiled Tesla’s (NASDAQ: TSLA) Master Plan Part 3, envisioning a long-term goal of nurturing a sustainable future without inflicting environmental harm. The plan anticipates a staggering $10 trillion investment, which Musk deems feasible and “not a big number relative to the global economy.”\\n\\nTesla is already diving into the lithium sector with substantial investments. In a previous earnings call, Musk revealed the company’s intent to set up a lithium refinery in Corpus Christi, Texas, to fortify control over its battery production supply chain. This proposed $375 million facility aims to convert raw materials into battery-grade lithium hydroxide, an essential component for Tesla’s EV batteries.\\n\\nDuring the Q&A segment of the 2023 Annual Shareholders’ Meeting, Musk shed light on Tesla’s peak margin upon achieving full autonomy. When asked to estimate Tesla’s anticipated peak margin in its automotive and energy businesses, Musk predicted an impressive 80% for Tesla’s automotive segment and a robust 20-30% for Tesla Energy.\\n\\nMusk’s vision is coming true by the day, with the company’s Model Y becoming the world’s best-selling vehicle in Q1 2023.\\n\\nYou probably already own Tesla stock by now, but why stop there? Take a step further and amplify your returns by investing more in Tesla and tapping into the affordable Arbor Metals as a lithium supplier to drive unmatched profit growth and future-proofing in the upcoming decades as the lithium and EV industries continue to grow exponentially.'),\n",
              " Document(metadata={'source': 'stock_market_june_2023/Mercado Libre.txt'}, page_content='MercadoLibre (NASDAQ: MELI), the Latin American e-commerce and fintech giant, has achieved its third consecutive all-time high for \\nquarterly net income in Q1 2023. Its stock price has surged by 57% this year, reflecting the company’s remarkable 58% sales \\ngrowth (adjusted for foreign exchange) and its decade-long success.\\nMercadoLibre holds a logistical dominance in Latin America similar to Amazon’s in the United States. Its shipping segment, Mercado Envíos, shipped over 302 million items, with 77% of products delivered within 48 hours. \\n\\nThis rapid delivery, coupled with a 93% penetration rate among sellers utilizing Envíos’ fulfillment and cross-docking solutions, underscores the compelling value proposition of partnering with MercadoLibre and its extensive logistical network.\\n\\nMercadoLibre’s valuation is estimated to be between its 18 times free cash flow (FCF) and its price-to-earnings (P/E) ratio of 107. With the rollout of a new advertising console in Q1, the company’s profitability is expected to improve further. Advertising sales, which currently account for only 1.4% of MercadoLibre’s gross merchandise volume (GMV), are projected to outpace GMV growth, contributing to sustained profitability and long-term stock returns.\\n'),\n",
              " Document(metadata={'source': 'stock_market_june_2023/Kiplin Metals Inc.txt'}, page_content='The global shift towards electric vehicles (EVs) is driving an exponential increase in electricity demand. Countries worldwide, including oil-producing nations, are turning to nuclear energy to meet this growing need while transitioning away from fossil fuels. Consequently, the demand for uranium, essential for fueling the nuclear power sector, has experienced a significant surge in the past year after decades of stagnation, clearly showcased in the uranium spot price.\\n\\nEveryone is drilling for uranium, and companies like Kiplin Metals Inc. (TSXV: KIP | FWB: 17G1 | OTC: ALDVF), which are strategically located in uranium-rich regions like Saskatchewan, Canada, stand to gain the most.\\nKiplin’s mining location next to Cluff Lake in Saskatchewan is a known vein of uranium that attracted companies like Urano in the past. The area has historically produced approximately 62,000,000 lbs of yellowcake uranium, making it a competitive place for other companies to buy claims. \\n\\nEvery mining company in Kiplin’s immediate area is continuing to discover rich uranium core samples, and experts predict that Kiplin Metals is poised to follow the success of previous neighbouring ventures as its summer exploration program gets underway. \\n\\nAs the demand for uranium remains strong, Kiplin is well-positioned to capitalize on the growing market, and its stock is primed for substantial returns.'),\n",
              " Document(metadata={'source': 'stock_market_june_2023/Arbor Metals Corp.txt'}, page_content='Arbor Metals Corp.\\nArbor Metals (TSX-V: ABR, FWB: 432, OTC: ABRMF) is a mineral exploration company that finds itself in the media spotlight as \\nCanada continues to attract massive investments across the lithium and electric vehicle (EV) sectors. \\nMajor media channels like Yahoo Finance, Financial Times, and CNN Business have recently focused on Arbor Metals Corp.’s strategic \\nlocation in the James Bay region of Quebec.\\nWith an impressive 126% annualized return over the past four years, Arbor’s shareholders have every reason to buy more and continue to hold onto their shares, especially given the company’s recent activities and the upcoming opportunities in the burgeoning Canadian lithium industry.\\n\\nThe company’s stock has had a historical trading volume increase over the last two months as expectations of its exploration program continue to be positive.\\n\\nAmbitious projects like Volkswagen’s 20 billion lithium-ion battery factory in Ontario signal massive growth for all local lithium mining stocks as the car makers actively seek lithium suppliers. Strategically located in a lithium-rich region of James Bay in northern Quebec, Arbor Metals’ Jarnet Lithium Project is positioned close to the planned factory.\\n\\nThe James Bay region boasts numerous lithium discoveries. One prime example is the lithium trend discovered on an adjacent property owned by Patriot Battery Metals (TSX-V: PMET). This lithium trend has been documented to extend toward Arbor’s Jarnet project, meaning Arbor will be the next to benefit from this discovery.\\n\\nThese developments prompted Arbor to enlist satellite imagery and infrared surveys to map and uncover new lithium deposits on their property. With a promising exploration program for 2023 underway, there are high hopes to unlock the new lithium resources, sharing this lithium growth trend with Patriot and other neighbouring sites.\\n\\nPatriot’s stock experienced a substantial surge of over 480% over the past year following the confirmation of its lithium discovery. Since industry geologists are even more optimistic about a larger find on Arbor’s property, analysts forecast a steep climb for Arbor’s stock in 2023, contingent on positive results from the company’s exploration program.\\n\\nArbor has the greatest upshot potential of all lithium companies right now and is great for short-term gains and long-term investors who would like to dependably profit from the growing EV market.'),\n",
              " Document(metadata={'source': 'stock_market_june_2023/Intuitive Surgical.txt'}, page_content='Intuitive Surgical (NASDAQ: ISRG) has dominated the surgical robotics industry for over two decades, maintaining a massive 80% market share. \\nIntuitive Surgical remains unrivalled despite other healthcare companies’ introduction of competing products.\\nThe global surgical robotics market is projected to reach nearly $100 billion by the next decade, driven by the increasing demand for these systems in minimally invasive and open surgeries. Intuitive Surgical is well-positioned to capitalize on this growth.\\n\\nThe industry faced significant challenges in recent years. The COVID-19 pandemic led to the cancellation and postponement of many surgical procedures. Resurgences in cases, particularly in Asia, continue to impact procedure volumes. Inflation and a shortage of healthcare workers have also raised operational costs for hospitals, affecting the adoption of surgical systems.\\n\\nDespite these obstacles, Intuitive Surgical has expanded revenue, maintained profitability, and increased system installations and customer training. The company generates revenue from surgical systems and recurring sources such as software, training, and customer support.\\n\\nIn Q1 2023, recurring revenue accounted for 81% of Intuitive Surgical’s $1.7 billion total revenue, showing a 21% increase compared to the previous year. Total revenue grew by 17% year over year on a currency-neutral basis, with a net income of $355 million.\\n\\nIntuitive Surgical has seen a 12% increase in its da Vinci Systems installed worldwide compared to the prior-year quarter. Despite the ongoing recovery of procedure volumes, the company’s resilient revenue model and steady growth demonstrate the strength of its business.')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split into chunks"
      ],
      "metadata": {
        "id": "TR66PCI4hmWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "iDkdhx8DcyuW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv_3blHic5EU",
        "outputId": "b86aa602-a953-4ff5-9a9f-bfe3a0260f89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZpUw707c78k",
        "outputId": "3b095662-99e9-41d3-8503-e23d63b34a3b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'stock_market_june_2023/Shopify.txt'}, page_content='To adapt to the changing landscape, Shopify implemented significant changes, including workforce layoffs of approximately 20% and selling its logistics business to Flexport. Although these changes incur near-term expenses, such as restructuring and impairment charges, they will ultimately reduce overhead costs and make Shopify more asset-light in the long run.')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the LLM"
      ],
      "metadata": {
        "id": "y5NxnrBjhqdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "kiqz06--dBBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Embeddings"
      ],
      "metadata": {
        "id": "YUgDrrrHhva7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "model = \"intfloat/e5-large-v2\"\n",
        "hf = HuggingFaceEmbeddings(model_name=model)"
      ],
      "metadata": {
        "id": "YZ0itXvXfKFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intialize Chromadb"
      ],
      "metadata": {
        "id": "NwH7ZmNvhy6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "persist_directory = 'db'\n",
        "\n",
        "embedding =hf\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "xFkYkpiFfoYe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb.persist()\n",
        "\n",
        "vectordb = None\n",
        "\n",
        "vectordb = Chroma(persist_directory=persist_directory,\n",
        "                  embedding_function=hf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEVWaAmwgWyF",
        "outputId": "d7e69fe2-4627-4e72-c1aa-28c53ef9d1dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-1530081794.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectordb.persist()\n",
            "/tmp/ipython-input-11-1530081794.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectordb = Chroma(persist_directory=persist_directory,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling LLM"
      ],
      "metadata": {
        "id": "YfkehY-ih7N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "kOF33CMFgsOK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "t2Vaj1Sxg2pz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" Could you please enumerate the companies that have been highlighted to their potential stock growth\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBNCSGfug-9g",
        "outputId": "1cdaf205-fc6e-421a-9999-f1dcc5a3c261"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-1064061156.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  llm_response = qa_chain(query)\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Arbor Metals Corp.\n",
            "Tesla Inc.\n",
            "Kiplin Metals Inc.\n",
            "Microsoft\n",
            "NVIDIA\n",
            "Shopify\n",
            "Intuitive Surgical\n",
            "MercadoLibre\n",
            "Growth stocks like these are ideal for investors seeking short-term gains and long-term resilience. These companies are primed to withstand future market fluctuations through innovation, market opportunities, embedding in recession-resistant sectors, and committing to diversifying their portfolios.\n",
            "\n",
            "Future-proofing your portfolio with growth stocks isn’t just about singling out products and industries that will always be in demand. \n",
            "It’s also about identifying companies engineered from inception to adapt to industry shifts. Microsoft (NASDAQ: MSFT) fits these criteria like a glove.\n",
            "\n",
            "Institutional ETFs assembled by Goldman Sachs and Vanguard rely on growth stocks to drive their price in all market conditions, and your portfolio is no different. Most of our recommendations are already a staple in these ETFs, but for investors looking to cherry-pick the best stocks, we have assembled this list for you.\n",
            "\n",
            "Here are 8 growth stocks for you to buy in June 2023 set to leverage emerging industry trends, such as AI, the lithium “gold” rush, blockbuster EV sales, and ESG.\n",
            "\n",
            "Question:  Could you please enumerate the companies that have been highlighted to their potential stock growth\n",
            "Helpful Answer: \n",
            "- Arbor Metals Corp. (NYSE: AMCI)\n",
            "- Tesla Inc. (NASDAQ: TSLA)\n",
            "- Kiplin Metals Inc. (NYSE: KPLM)\n",
            "- Microsoft Corp. (NASDAQ: MSFT)\n",
            "- NVIDIA Corp. (NASDAQ: NVDA)\n",
            "- Shopify Inc. (NYSE: SHOP)\n",
            "- Intuitive Surgical Inc. (NASDAQ: ISRG)\n",
            "- MercadoLibre Inc. (NASDAQ: MELI)\n",
            "- Growth Stock ETFs: Vanguard Growth ETF (VUG), iShares Core Growth ETF (IWO), and Fidelity Contrafund Growth (FNGLX)\n",
            "\n",
            "\n",
            "Sources:\n",
            "stock_market_june_2023/stock_market.txt\n",
            "stock_market_june_2023/Microsoft Corporation.txt\n",
            "stock_market_june_2023/stock_market.txt\n"
          ]
        }
      ]
    }
  ]
}