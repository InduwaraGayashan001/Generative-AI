{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InduwaraGayashan001/Generative-AI/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48L_2WzUCOdq"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qJV5FJF99Hsv",
        "outputId": "f85b6cf6-0d48-4bdd-91ca-39a78150e827"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain_community\n",
        "!pip -q install pypdf\n",
        "!pip -q install sentence_transformers\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install tokenizers\n",
        "!pip install faiss-cpu\n",
        "!pip -q install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzdHrX9N-DgP",
        "outputId": "298d4811-4581-4061-e7e8-6994f5913c22"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.4\n",
        "!pip install nltk==3.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LWUGinfS_rNK",
        "outputId": "7be599c4-bae5-477d-86f6-76c497b66b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.22-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.64 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.65)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.84.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.64->langchain_openai) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.64->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.64->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.64->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.64->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.64->langchain_openai) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.64->langchain_openai) (2.11.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.64->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.64->langchain_openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.64->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.64->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.64->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.64->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.64->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.4.0)\n",
            "Downloading langchain_openai-0.3.22-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.22\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5lLI5eav-mBn"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQAWithSourcesChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFUZI0UzAF4m",
        "outputId": "4aad67e7-cde3-4e80-d923-c46fe1d587a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "voY2CXIHAagZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('GITHUB_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQtIxq6BCTi1"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7LAf6r7IBd6L"
      },
      "outputs": [],
      "source": [
        "URLs =[\n",
        "    'https://medium.com/@srikar.appal/paper-review-2-multi-digit-number-recognition-from-street-view-imagery-using-deep-convolutional-c915f2bdde67',\n",
        "    'https://lmsys.org/blog/2023-03-30-vicuna/',\n",
        "    'https://stability.ai/research/facecraft4d-animated-3d-facial-avatar-generation-from-a-single-image'\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vTt262xiB-Id"
      },
      "outputs": [],
      "source": [
        "loaders = UnstructuredURLLoader(urls=URLs)\n",
        "data = loaders.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MzhbXFSTCD24",
        "outputId": "e373d463-c321-4018-be96-467a4cf1c76a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://medium.com/@srikar.appal/paper-review-2-multi-digit-number-recognition-from-street-view-imagery-using-deep-convolutional-c915f2bdde67'}, page_content='Sitemap\\n\\nSign in\\n\\nWrite\\n\\nSign in\\n\\nPaper Review 2 — Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks\\n\\nSrikar Appalaraju\\n\\nSrikar Appalaraju\\n\\n3 min read\\n\\nApr 13, 2018\\n\\n--\\n\\nThis paper published in ICLR 2014 is again a rather old paper but following up on the last reviewed paper this paper follows up on it. The authors show that using a CNN they were able to automate street number extraction from street images.\\n\\nTraditional OCR techniques to extract text from images is performed in three stages — localization (figure out where in the image the text is), segmentation (segment out the individual characters from the localized part of the image), recognition (recognize the segmented characters). Traditionally all these three stages were done independently by hand-crafted features. In this work the authors show that a CNN was used to automate all these stages and it achieved new state of the art performance. This work roughly follows the work done by Andre Ng et. al.\\n\\nData\\n\\nThe authors use the SVHN dataset and also a larger google internal dataset. the SVHN dataset consists of little over 73K images for train and 26K images for test. It also has a larger extra dataset of 531K images. The way this data was generated was covered in detail in the earlier blog post and also in the original paper.\\n\\nData preprocessing was minimal — 1) subtract mean of each image 2) no whitening or local contrast normalization was used 3) data augmentation was used on SVHN dataset. they seem to have used a form of “cropping” as a primary way to increase data. Instead of random cropping (which could generate images without any house numbers in them), they devised a way to do cropping wherein digits were present in the final resulting image.\\n\\nMethodology\\n\\nThe authors use a multi-layer deep CNN which takes input as the image and spits out the conditional probabilistic output of the sequences. Its a pretty standard CNN, although it’s interesting to see the output layer being of a different kind. Also the intuition of using max-out units for SVHN dataset in the first hidden layer is unclear. Perhaps they better capture initial edge detections?\\n\\nAnother interesting aspect of the paper is how the problem was framed. They used model Accuracy @ coverage as a metric. They call it confidence thresholding. The authors go on to explain several advantages of this technique for their business use-case. Also the main goal is to minimize incorrect transcriptions. Credit was given to the model only for correctly predicting ALL digits in the image, no “partial-credit” given. It was acceptable NOT to transcript EVERY input image. It was clear with this that the authors clearly did not want False-Positives in their predictions. Minimizing FP. They are okay with more True-Negatives. Misses in transcript are acceptable but no False-positives are acceptable.\\n\\nLimitations and Unclear areas\\n\\nThe authors claim to have solved OCR for short sequences i.e. sequences with fixed N.\\n\\nNo mention of Class imbalance. The traditional techniques of evaluating a classifier are absent. i.e. the chance of finding a house number in a random image from the dataset.\\n\\nNot sure why “accuracy” is a better measure here? Why was Precision-Recall not used?\\n\\nResults\\n\\nthis methodology (which has become mainstream in 2018) results in 98% accuracy at 95.64% coverage on SVHN dataset and 99% accuracy at 83% coverage in their Internal street view data. Following are the authors hypothesis for the CNN’s exceptional performance (note most of these are common knowledge now with several experimental and theoretical works supporting these observations) —\\n\\nDeeper the CNN layers, better the performance.\\n\\nEarly layers help CNN do the Localization + Segmentation tasks and prepare a “representation” for the later layers to do Recognition on.\\n\\nDeeper the network, more the learning capacity of the model hence more the training data needed.\\n\\nReferences\\n\\nBlog post trying this out in tensorflow — http://matthewearl.github.io/2016/05/06/cnn-anpr/\\n\\nMachine Learning\\n\\nComputer Vision\\n\\nDeep Learning\\n\\nGoogle\\n\\nPaper Review\\n\\n--\\n\\n--\\n\\nSrikar Appalaraju\\n\\nSrikar Appalaraju\\n\\nWritten by Srikar Appalaraju\\n\\n18 followers\\n\\n9 following\\n\\nNo responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech'),\n",
              " Document(metadata={'source': 'https://lmsys.org/blog/2023-03-30-vicuna/'}, page_content='LMSYS ORG\\n\\nProjectsBlogAboutDonationsChatbot Arena (graduated)\\n\\nVicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality\\n\\nby: The Vicuna Team, Mar 30, 2023\\n\\nWe introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%* of cases. The cost of training Vicuna-13B is around $300. The code and weights, along with an online demo, are publicly available for non-commercial use.\\n\\n\\n\\nVicuna (generated by stable diffusion 2.1)\\n\\nAccording to a fun and non-scientific evaluation with GPT-4. Further rigorous evaluation is needed.\\n\\nHow Good is Vicuna?\\n\\nAfter fine-tuning Vicuna with 70K user-shared ChatGPT conversations, we discover that Vicuna becomes capable of generating more detailed and well-structured answers compared to Alpaca (see examples below), with the quality on par with ChatGPT.\\n\\nHowever, evaluating chatbots is never a simple task. With recent advancements in GPT-4, we are curious whether its capabilities have reached a human-like level that could enable an automated evaluation framework for benchmark generation and performance assessments. Our initial finding indicates that GPT-4 can produce highly consistent ranks and detailed assessment when comparing chatbots’ answers (see above example of GPT-4 judgment). Preliminary evaluations based on GPT-4, summarized in Figure 1, show that Vicuna achieves 90%* capability of Bard/ChatGPT. While this proposed framework shows a potential to automate chatbot assessment, it is not yet a rigorous approach. Building an evaluation system for chatbots remains an open question requiring further research. More details are provided in the evaluation section.\\n\\n\\n\\nFigure 1. Relative Response Quality Assessed by GPT-4*\\n\\nOnline Demo\\n\\nTry the Vicuna-13B demo here!\\n\\nOverview\\n\\nThe rapid advancement of large language models (LLMs) has revolutionized chatbot systems, resulting in unprecedented levels of intelligence as seen in OpenAI\\'s ChatGPT. However, despite its impressive performance, the training and architecture details of ChatGPT remain unclear, hindering research and open-source innovation in this field. Inspired by the Meta LLaMA and Stanford Alpaca project, we introduce Vicuna-13B, an open-source chatbot backed by an enhanced dataset and an easy-to-use, scalable infrastructure. By fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.com, Vicuna-13B has demonstrated competitive performance compared to other open-source models like Stanford Alpaca. This blog post provides a preliminary evaluation of Vicuna-13B\\'s performance and describes its training and serving infrastructure. We also invite the community to interact with our online demo to test the capabilities of this chatbot.\\n\\n\\n\\nFigure 2. Workflow Overview\\n\\nFigure 2 provides an overview of our work. To begin, we collected around 70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations. Next, we enhanced the training scripts provided by Alpaca to better handle multi-turn conversations and long sequences. The training was done with PyTorch FSDP on 8 A100 GPUs in one day. For serving the demo, we implemented a lightweight distributed serving system. We conducted a preliminary evaluation of the model quality by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs. To compare two different models, we combine the outputs from each model into a single prompt for each question. The prompts are then sent to GPT-4, which assesses which model provides better responses. A detailed comparison of LLaMA, Alpaca, ChatGPT, and Vicuna is shown in Table 1 below.\\n\\nTable 1. Comparison between several notable models\\n\\nModel Name LLaMA Alpaca Vicuna Bard/ChatGPT Dataset Publicly available datasets (1T token) Self-instruct from davinci-003 API (52K samples) User-shared conversations (70K samples) N/A Training code N/A Available Available N/A Evaluation metrics Academic benchmark Author evaluation GPT-4 assessment Mixed Training cost (7B) 82K GPU-hours $500 (data) + $100 (training) $140 (training) N/A Training cost (13B) 135K GPU-hours N/A $300 (training) N/A\\n\\nTraining\\n\\nVicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model\\'s maximum context length.\\n\\nOur training recipe builds on top of Stanford’s alpaca with the following improvements.\\n\\nMulti-turn conversations: We adjust the training loss to account for multi-turn conversations and compute the fine-tuning loss solely on the chatbot\\'s output.\\n\\nMemory Optimizations: To enable Vicuna\\'s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing gradient checkpointing and flash attention.\\n\\nCost Reduction via Spot Instance: The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ SkyPilot managed spot to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.\\n\\nServing\\n\\nWe build a serving system that is capable of serving multiple models with distributed workers. It supports flexible plug-in of GPU workers from both on-premise clusters and the cloud. By utilizing a fault-tolerant controller and managed spot feature in SkyPilot, this serving system can work well with cheaper spot instances from multiple clouds to reduce the serving costs. It is currently a lightweight implementation and we are working on integrating more of our latest research into it.\\n\\nHow To Evaluate a Chatbot?\\n\\nEvaluating AI chatbots is a challenging task, as it requires examining language understanding, reasoning, and context awareness. With AI chatbots becoming more advanced, current open benchmarks may no longer suffice. For instance, the evaluation dataset used in Stanford’s Alpaca, self-instruct, can be effectively answered by SOTA chatbots, making it difficult for humans to discern differences in performance. More limitations include training/test data contamination and the potentially high cost of creating new benchmarks. To tackle these issues, we propose an evaluation framework based on GPT-4 to automate chatbot performance assessment.\\n\\nFirst, we devised eight question categories, such as Fermi problems, roleplay scenarios, and coding/math tasks, to test various aspects of a chatbot\\'s performance. Through careful prompt engineering, GPT-4 is able to generate diverse, challenging questions that baseline models struggle with. We select ten questions per category and collect answers from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. We then ask GPT-4 to rate the quality of their answers based on helpfulness, relevance, accuracy, and detail. We discover that GPT-4 can produce not only relatively consistent scores but also detailed explanations on why such scores are given (detailed examples link). However, we also notice that GPT-4 is not very good at judging coding/math tasks.\\n\\n\\n\\nFigure 3. Response Comparison Assessed by GPT-4\\n\\nFigure 3 displays the comparison results between all baselines and Vicuna. GPT-4 prefers Vicuna over state-of-the-art open-source models (LLaMA, Alpaca) in more than 90% of the questions, and it achieves competitive performance against proprietary models (ChatGPT, Bard). In 45% of the questions, GPT-4 rates Vicuna\\'s response as better or equal to ChatGPT\\'s. As GPT-4 assigns a quantitative score to each response on a scale of 10, we calculate the total score for each (baseline, Vicuna) comparison pair by adding up the scores obtained by each model on 80 questions. As shown in Table 2, Vicuna’s total score is 92% of ChatGPT’s. Despite recent advancements, these chatbots still face limitations, such as struggling with basic math problems or having limited coding ability.\\n\\nTable 2. Total Scores Assessed by GPT-4.\\n\\nBaseline Baseline Score Vicuna Score LLaMA-13B 513.0 694.0 Alpaca-13B 583.0 704.0 Bard 664.0 655.5 ChatGPT 693.0 638.0\\n\\nWhile this proposed evaluation framework demonstrates the potential for assessing chatbots, it is not yet a rigorous or mature approach, as large language models are prone to hallucinate. Developing a comprehensive, standardized evaluation system for chatbots remains an open question requiring further research.\\n\\nEdited: After this blog post, we conducted a deeper study on this GPT4-based evaluation approach. You are welcome to read our new Judging LLM-as-a-judge paper and try the new evaluation tool.\\n\\nLimitations\\n\\nWe have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI moderation API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.\\n\\nRelease\\n\\nIn our first release, we will share the training, serving, and evaluation code on a GitHub repo: https://github.com/lm-sys/FastChat. We also released the Vicuna-13B model weights. There is no plan to release the dataset. Join our Discord server and follow our Twitter to get the latest updates.\\n\\nLicense\\n\\nThe online demo is a research preview intended for non-commercial use only, subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. Please contact us If you find any potential violation. The code is released under the Apache License 2.0.\\n\\nAcknowledgment\\n\\nWe would like to thank Xinyang Geng, Hao Liu, and Eric Wallace from BAIR; Xuecheng Li, and Tianyi Zhang from Stanford Alpaca team for their insightful discussion and feedback; Qirong Ho from MBZUAI for providing support on the serving cluster. Please check out a blog post from BAIR about a concurrent effort on their chatbot, Koala.\\n\\nThe Team\\n\\nThis is a joint effort with collaborators from multiple institutions, including UC Berkeley, CMU, Stanford, UC San Diego, and MBZUAI.\\n\\nStudents (alphabetical order): Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang (✉), Lianmin Zheng (✉), Siyuan Zhuang, Yonghao Zhuang\\n\\nAdvisors (alphabetical order): Joseph E. Gonzalez, Ion Stoica, Eric P. Xing\\n\\n✉ Correspondence to: Lianmin Zheng (lianminzheng@gmail.com), Hao Zhang (sjtu.haozhang@gmail.com), or LMSYS (lmsys.org@gmail.com).\\n\\nCitation\\n\\n@misc{vicuna2023,\\n    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\\\%* ChatGPT Quality},\\n    url = {https://lmsys.org/blog/2023-03-30-vicuna/},\\n    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},\\n    month = {March},\\n    year = {2023}\\n}\\n\\nAfter this blog post, we extended our idea of GPT-4 based evaluation and wrote a more formal paper that systematically studies this \"LLM-as-a-judge\" approach. You are welcome to read and cite this paper: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.'),\n",
              " Document(metadata={'source': 'https://stability.ai/research/facecraft4d-animated-3d-facial-avatar-generation-from-a-single-image'}, page_content='FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image\\n\\nApr 21\\n\\nWritten By Savannah Martin\\n\\nWe present a novel framework for generating high-quality, animatable 4D avatar from a single image. While recent advances have shown promising results in 4D avatar creation, existing methods either require extensive multiview data or struggle with shape accuracy and identity consistency. To address these limitations, we propose a comprehensive system that leverages shape, image, and video priors to create full-view, animatable avatars. Our approach first obtains initial coarse shape through 3D-GAN inversion. Then, it enhances multiview textures using depth-guided warping signals for cross-view consistency with the help of the image diffusion model. To handle expression animation, we incorporate a video prior with synchronized driving signals across viewpoints. We further introduce a Consistent-Inconsistent training to effectively handle data inconsistencies during 4D reconstruction. Experimental results demonstrate that our method achieves superior quality compared to the prior art, while maintaining consistency across different viewpoints and expressions.\\n\\nRead the paper\\n\\n\\n\\nSavannah Martin\\n\\nPrevious\\n\\nPrevious\\n\\nFast Text-to-Audio Generation with Adversarial Post-Training\\n\\nNext\\n\\nNext\\n\\nSV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVEeF8PqCGkc",
        "outputId": "7b8a306b-3785-4666-9377-17a436ad6956"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8k7nawHCZ9w"
      },
      "source": [
        "# Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ry7MsUiGCJeM"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(separator='\\n',chunk_size=1000, chunk_overlap=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQPMqT6mC2D3",
        "outputId": "c4dd1f84-b21c-4a0e-e5d6-06be6714c3d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1072, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSPRz5jSC890",
        "outputId": "ed2dc9fb-9fdf-4d28-b2b8-44e006788878"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR0MqWXgDHDa"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n1XIEDANDFi9"
      },
      "outputs": [],
      "source": [
        "endpoint = \"https://models.inference.ai.azure.com\"\n",
        "model_name = \"text-embedding-3-small\"\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model = model_name,\n",
        "    openai_api_base = endpoint,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dUWDmJkD4X3",
        "outputId": "67d1848e-471f-4d4b-dab2-47c31859ecae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lW12O2NE96E"
      },
      "source": [
        "# Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wi0Wdz_RELao"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_documents(text_chunks, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ7d8T0XFDib"
      },
      "source": [
        "# Integrate with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OTvUvlXZEYgU"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "             model = \"openai/gpt-4o-mini\",\n",
        "             api_key=userdata.get('GITHUB_TOKEN'),\n",
        "             base_url=\"https://models.github.ai/inference\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "sQzZ_zdbEu8M",
        "outputId": "e2d047eb-f27a-4265-8eb0-eeec917009cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-959901977>:1: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  llm.predict(\"What is capital of France\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The capital of France is Paris.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.predict(\"What is capital of France\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "g6G6t-9yFHTS"
      },
      "outputs": [],
      "source": [
        "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7B94dudFcez",
        "outputId": "de422f59-15da-45d4-be3f-5180a20cee7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-3636243516>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = chain({\"question\": \"How good is Vicuna?\"}, return_only_outputs=True)\n"
          ]
        }
      ],
      "source": [
        "result = chain({\"question\": \"How good is Vicuna?\"}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "BdeFXMhGFtCH",
        "outputId": "771d99d4-30e9-41e2-f8ce-ce8f1ef489bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Vicuna is capable of generating detailed and well-structured answers, achieving more than 90% quality of outputs compared to OpenAI's ChatGPT and Google Bard. It has been evaluated favorably against other models, outperforming them in over 90% of cases. However, like other large language models, Vicuna has limitations, particularly with reasoning, mathematics, and ensuring the factual accuracy of its outputs.\\n\\n\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miaAt0PzHjX4"
      },
      "source": [
        "# Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "w2lKG6T2GH4Y",
        "outputId": "b03d1571-64cf-49f4-8dcf-8ccfd2370592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: What is Vicuna?\n",
            "Answer: Vicuna is an open-source chatbot that has been fine-tuned from the LLaMA base model using user-shared conversations collected from ShareGPT. It is designed to generate detailed and well-structured answers, achieving over 90% quality compared to OpenAI's ChatGPT and Google Bard, while outperforming other models in most cases. The training of Vicuna-13B cost around $300, and its code and weights are publicly available for non-commercial use.\n",
            "\n",
            "\n",
            "Prompt: How can we generate a 3D avatar model by single image?\n",
            "Answer: A novel framework for generating high-quality, animatable 4D avatars from a single image has been proposed. This system utilizes shape, image, and video priors to create full-view, animatable avatars. Initially, a coarse shape is obtained through 3D-GAN inversion, followed by enhancing multiview textures for consistency using depth-guided warping signals. The framework also incorporates a video prior for expression animation, ensuring synchronization across viewpoints. This comprehensive approach enables effective handling of data inconsistencies during the 4D reconstruction process.\n",
            "\n",
            "\n",
            "Prompt: \n",
            "Prompt: exit()\n",
            "Answer: I don't know.  \n",
            "\n",
            "Prompt: exit\n",
            "Exiting\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  query=input(f\"Prompt: \")\n",
        "  if query == \"exit\":\n",
        "    print(\"Exiting\")\n",
        "    sys.exit()\n",
        "  if query == \"\":\n",
        "    continue\n",
        "  result = chain({\"question\": query}, return_only_outputs=True)\n",
        "  print(f\"Answer: {result['answer']}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP4cQvksklBa5g47mwVx4xD",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
